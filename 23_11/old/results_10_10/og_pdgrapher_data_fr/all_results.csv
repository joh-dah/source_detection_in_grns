filename,model_type,network,num_nodes,num_possible_sources,avg_num_sources,avg_portion_affected,accuracy,avg rank of source,avg prediction for source,avg prediction over all nodes,min prediction over all nodes,max prediction over all nodes,source in top 3,source in top 5,source in top 20,source in top 40,source in top 80,pdgrapher_perturbation_discovery,pdgrapher_response_prediction
pdgrapher_1002_0019,pdgrapher,random_10716_300000,,,,,0.015,1908.387,0.927,-0.89,-5.031,6.382,0.029,0.036,,,,,
pdgrapher_1002_0838,pdgrapher,random_10716_300000,,,,,0.014,3569.96,0.116,-0.019,-0.948,3.844,0.023,0.029,,,,,
pdgrapher_1008_1236,pdgrapher,random_10716_300000,,,,,0.014,2031.176,0.545,-0.158,-2.898,5.523,0.029,0.034,,,,,
pdgrapher_1008_1606,pdgrapher,random_10716_300000,,,,,0.013,2448.741,0.398,-0.19,-2.383,5.609,,,0.048,0.062,0.083,,
pdgrapher_1009_1317,pdgrapher,random_10716_300000,,,,,0.012,2523.52,0.439,-0.152,-2.627,7.062,,0.032,0.058,0.075,0.098,"{'recall@1': 0.0054, 'recall@10': 0.0223, 'recall@100': 0.0563, 'recall@1000': 0.2758, 'ranking_score': 0.7195, 'ranking_score_dcg': 0.0835, 'perc_partially_accurate_predictions': 0.54, 'recall@1_std': 0.073, 'recall@10_std': 0.1475, 'recall@100_std': 0.2305, 'recall@1000_std': 0.4469, 'ranking_score_std': 0.2484, 'ranking_score_dcg_std': 0.0922, 'perc_partially_accurate_predictions_std': 7.3, 'total_samples': 4851, 'avg_topk': 7710.4857}","{'forward_mae': -1, 'forward_mse': -1, 'forward_r2': -1, 'forward_r2_scgen': -1, 'forward_spearman': -1, 'backward_mae': -1, 'backward_mse': -1, 'backward_r2': -1, 'backward_r2_scgen': -1, 'backward_spearman': -1, 'backward_avg_topk': -1}"
pdgrapher_1009_1506,pdgrapher,random_10716_300000,,,,,0.012,2523.52,0.439,-0.152,-2.627,7.062,,0.032,0.058,0.075,0.098,"{'recall@1': 0.0025, 'recall@10': 0.0233, 'recall@100': 0.0629, 'recall@1000': 0.2645, 'ranking_score': 0.7189, 'ranking_score_dcg': 0.0823, 'perc_partially_accurate_predictions': 0.25, 'recall@1_std': 0.0497, 'recall@10_std': 0.1508, 'recall@100_std': 0.2427, 'recall@1000_std': 0.4411, 'ranking_score_std': 0.2464, 'ranking_score_dcg_std': 0.082, 'perc_partially_accurate_predictions_std': 4.97, 'total_samples': 4851, 'avg_topk': 7703.8955}","{'forward_mae': -1, 'forward_mse': -1, 'forward_r2': -1, 'forward_r2_scgen': -1, 'forward_spearman': -1, 'backward_mae': -1, 'backward_mse': -1, 'backward_r2': -1, 'backward_r2_scgen': -1, 'backward_spearman': -1, 'backward_avg_topk': -1}"
